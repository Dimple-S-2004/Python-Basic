{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Supervised Classification: Decision Trees, SVM, and Naive Bayes"
      ],
      "metadata": {
        "id": "y1G-1fA_9_kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "-->  Information Gain is a key concept used in **Decision Tree algorithms** to select the best feature for splitting the dataset at each node.  \n",
        "It measures how much **uncertainty is reduced** in the target variable after splitting the data using a particular attribute.\n",
        "Decision tree algorithms such as **ID3** use Information Gain to build an optimal tree.\n",
        "\n",
        "**Information Gain (IG)** is the **reduction in entropy** achieved after splitting a dataset on an attribute.\n",
        "\n",
        "In simple terms: Information Gain tells us how useful a feature is in classifying the data.\n",
        "\n",
        "A feature with **higher Information Gain** is selected for splitting.\n",
        "\n",
        "Information Gain Formula:\n",
        "$\n",
        "Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- \\(S\\) = total dataset  \n",
        "- \\(A\\) = attribute  \n",
        "- $S_v$ = subset after splitting on attribute \\(A\\)  \n",
        "- $|S_v| / |S|$ = weight of subset  \n",
        "\n",
        "\n",
        "**Steps involved in using Information Gain:**\n",
        "* 1. Calculate the entropy of the entire dataset.\n",
        "* 2. For each feature:\n",
        "   - Split the dataset based on feature values.\n",
        "   - Calculate entropy of each subset.\n",
        "   - Compute the weighted average entropy.\n",
        "* 3. Calculate Information Gain for each feature.\n",
        "* 4. Choose the feature with **maximum Information Gain**.\n",
        "* 5. Repeat the process recursively until:\n",
        "   - All samples belong to the same class, or\n",
        "   - No features remain.\n",
        "\n",
        "*Example* :\n",
        "Suppose we want to predict whether a person will **play cricket** based on weather conditions.\n",
        "\n",
        "If a feature like **Outlook** splits the data such that each subset contains only one class, then:\n",
        "- Entropy becomes **0**\n",
        "- Information Gain becomes **maximum**\n",
        "\n",
        "Hence, that feature is chosen as the **root node**.\n",
        "\n",
        "\n",
        "Importance of Information Gain:\n",
        "- Helps in selecting the most important feature\n",
        "- Reduces uncertainty in classification\n",
        "- Improves prediction accuracy\n",
        "- Makes the decision tree simple and interpretable\n",
        "\n",
        "\n",
        "Advantages of Information Gain:\n",
        "1. Easy to understand and implement\n",
        "2. Works well with categorical data\n",
        "3. Helps build efficient decision trees\n",
        "4. Provides clear decision rules\n",
        "\n",
        "\n",
        "Algorithms Using Information Gain\n",
        "- **ID3** – Uses Information Gain\n",
        "- **C4.5** – Uses Gain Ratio (improvement over IG)\n",
        "- **CART** – Uses Gini Index instead of IG\n",
        "\n",
        "\n",
        "*Conclusion*:\n",
        "Information Gain is an important metric in Decision Trees that helps select the best attribute by reducing entropy. It plays a vital role in constructing efficient and accurate classification models and is widely used in machine learning.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dElhTq5I-SW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "--> Gini Impurity and Entropy are two commonly used impurity measures in **Decision Tree algorithms**.  \n",
        "They help determine how well a feature splits the dataset by measuring the level of impurity or randomness in the data.\n",
        "\n",
        "Both aim to create **pure child nodes**, but they differ in formulation, interpretation, and computational efficiency.\n",
        "\n",
        "Definitions:\n",
        "\n",
        "**Entropy**  \n",
        "Entropy measures the **amount of uncertainty or randomness** in a dataset.  \n",
        "It is derived from **Information Theory** and is used to calculate **Information Gain**.\n",
        "\n",
        "**Gini Impurity**  \n",
        "Gini Impurity measures the **probability of incorrectly classifying** a randomly chosen data point if it were assigned a label based on the class distribution of the node.\n",
        "\n",
        "\n",
        "\n",
        " **Formulas**:\n",
        "\n",
        " -Entropy Formula:\n",
        "\n",
        "$\n",
        "Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "$\n",
        "\n",
        "-  Gini Impurity Formula:\n",
        "\n",
        "$\n",
        "Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "$\n",
        "\n",
        "Where:\n",
        "- \\(S\\) = dataset  \n",
        "- \\(c\\) = number of classes  \n",
        "- $p_i$ = probability of class $i$\n",
        "\n",
        "\n",
        "**Range of Values**:\n",
        "\n",
        "- **Entropy** ranges from **0 to 1** (for binary classification)\n",
        "- **Gini Impurity** ranges from **0 to 0.5** (for binary classification)\n",
        "\n",
        "Value **0** indicates a pure node, while higher values indicate more impurity.\n",
        "\n",
        "\n",
        "**Working Principle**:\n",
        "- Entropy measures the **expected information required** to classify a data point.\n",
        "- Gini Impurity measures the **likelihood of misclassification**.\n",
        "\n",
        "Both measures try to **minimize impurity after splitting**.\n",
        "\n",
        "\n",
        "**Difference between Entropy and Gini Impurity**:\n",
        "\n",
        "| Aspect | Entropy | Gini Impurity |\n",
        "|-----|--------|---------------|\n",
        "| Concept | Information content | Misclassification probability |\n",
        "| Formula | Uses logarithm | Uses squared probabilities |\n",
        "| Computation | Slower | Faster |\n",
        "| Sensitivity | More sensitive to class changes | Less sensitive |\n",
        "| Bias | Less biased | Slight bias toward majority class |\n",
        "| Tree Quality | Slightly better splits | Comparable splits |\n",
        "| Algorthim | ID3, C4.5 (via Information Gain / Gain Ratio)|CART|\n",
        "| Dataset |Small dataset | Large dataset |\n",
        "\n",
        "\n",
        "**Example (Conceptual)** :\n",
        "\n",
        "If a node contains only one class:\n",
        "- Entropy = 0  \n",
        "- Gini Impurity = 0  \n",
        "\n",
        "If a node contains two classes equally (50–50):\n",
        "- Entropy = 1  \n",
        "- Gini Impurity = 0.5  \n",
        "\n",
        "This indicates maximum impurity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Entropy\n",
        "\n",
        "**Strengths**\n",
        "* Strong theoretical foundation\n",
        "* Produces balanced trees\n",
        "\n",
        "**Weaknesses**\n",
        "*  Computationally expensive\n",
        "*   Slower for large datasets\n",
        "\n",
        "2. Gini Impurity\n",
        "\n",
        "**Strengths**\n",
        "* Faster computation\n",
        " * Efficient for large datasets\n",
        "\n",
        "**Weaknesses**\n",
        "* Slightly less informative\n",
        "*  Can favor majority classes\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "Both Gini Impurity and Entropy are effective measures for building decision trees.  \n",
        "Entropy provides a strong information-theoretic basis, while Gini Impurity is computationally efficient.  \n",
        "In practice, both often give similar results, and the choice depends on dataset size and performance needs.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "boBCCqQfz3jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "--> Pre-Pruning (also called early stopping) is a technique used in Decision Trees to stop the tree from growing further before it becomes too complex.\n",
        "In pre-pruning, the algorithm decides in advance when to stop splitting a node, even if further splits are possible.\n",
        "\n",
        "**Pre-Pruning is Needed for:**\n",
        "\n",
        "1. Decision trees tend to overfit the training data by:\n",
        "\n",
        "* Creating very deep trees\n",
        "\n",
        "* Learning noise instead of patterns\n",
        "\n",
        "2. Pre-pruning helps to:\n",
        "\n",
        "* Reduce overfitting\n",
        "\n",
        "* Improve generalization on unseen data\n",
        "\n",
        "* Control tree size\n",
        "\n",
        "**How Pre-Pruning Works:** During tree construction, splitting is stopped if certain conditions are met.Common stopping criteria include:\n",
        "\n",
        "1. Maximum depth reached\n",
        "\n",
        "* Stop splitting if tree depth exceeds a fixed limit\n",
        "\n",
        "2.  Minimum samples at a node\n",
        "\n",
        "* Do not split if the number of samples is too small\n",
        "\n",
        "3. Minimum information gain\n",
        "\n",
        "* Stop splitting if Information Gain or Gini reduction is below a threshold\n",
        "\n",
        "4. Maximum number of leaf nodes\n",
        "* Restrict the number of terminal nodes\n",
        "\n",
        " **Example**:\n",
        "\n",
        "Suppose we are building a decision tree to predict whether a person will buy a product.\n",
        "If a node has only 5 samples and splitting it does not significantly reduce impurity, then:\n",
        "\n",
        "The algorithm stops splitting\n",
        "\n",
        "That node becomes a leaf node\n",
        "\n",
        "This prevents the model from learning noise.\n",
        "\n",
        "**Advantages of Pre-Pruning**:\n",
        "\n",
        "* Prevents overfitting\n",
        "\n",
        "* Reduces training time\n",
        "\n",
        "* Produces simpler and smaller trees\n",
        "\n",
        "* Improves model generalization\n",
        "\n",
        "**Disadvantages of Pre-Pruning**:\n",
        "\n",
        "* Risk of underfitting\n",
        "\n",
        "* Important patterns may be missed\n",
        "\n",
        "* Requires careful parameter tuning\n",
        "\n",
        "* Decisions are made without seeing full tree structure\n",
        "\n",
        "\n",
        "**Conclusion**:\n",
        "Pre-Pruning is an effective method to control the growth of decision trees by stopping splits early. It helps reduce overfitting and computational cost but may sometimes lead to underfitting if applied too aggressively. Hence, it should be used carefully with proper parameter selection.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CcEDG11_4QHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "#Code:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Train Decision Tree with Gini Impurity\n",
        "dt = DecisionTreeClassifier(criterion='gini')\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "#Print feature importances\n",
        "print(\"Feature Importances:\", dt.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knknd1h97Xgf",
        "outputId": "668f0b4b-03f5-4208-faac-c8b6cbf0830c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01911002 0.         0.42356658 0.5573234 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "-->Support Vector Machine (SVM) is a **supervised machine learning algorithm** used for **classification and regression** tasks.  \n",
        "It is mainly used for classification problems and is known for its effectiveness in handling **high-dimensional data**.\n",
        "\n",
        "SVM works by finding an **optimal decision boundary (hyperplane)** that best separates different classes in the dataset.\n",
        "\n",
        "A **Support Vector Machine** is a learning algorithm that finds the **maximum margin hyperplane** that separates data points of different classes with the largest possible margin.\n",
        "\n",
        "The data points that lie closest to the hyperplane are called **support vectors**.  \n",
        "These points play a crucial role in defining the position and orientation of the hyperplane.\n",
        "\n",
        "\n",
        "\n",
        "** Key Concepts in SVM**:\n",
        "\n",
        "-(a) Hyperplane\n",
        "A **hyperplane** is a decision boundary that separates different classes.\n",
        "- In 2D → a line  \n",
        "- In 3D → a plane  \n",
        "- In higher dimensions → a hyperplane  \n",
        "\n",
        "-(b) Margin\n",
        "The **margin** is the distance between the hyperplane and the nearest data points from each class.  \n",
        "SVM aims to **maximize this margin**.\n",
        "\n",
        "-(c) Support Vectors\n",
        "Support vectors are the **closest data points to the hyperplane**.  \n",
        "They directly influence the position of the decision boundary.\n",
        "\n",
        "**Working Principle of SVM**:\n",
        "1. Identify all possible separating hyperplanes.\n",
        "2. Select the hyperplane with the **maximum margin**.\n",
        "3. Classify new data points based on which side of the hyperplane they lie.\n",
        "\n",
        "This makes SVM robust and less sensitive to noise.\n",
        "\n",
        "\n",
        "**Mathematical Representation**:\n",
        "The equation of a hyperplane is given by:\n",
        "\n",
        "$\n",
        "w \\cdot x + b = 0\n",
        "$\n",
        "\n",
        "Where:\n",
        "- w = weight vector  \n",
        "- x = input feature vector  \n",
        "- b = bias  \n",
        "\n",
        "The goal of SVM is to find w and b such that the margin is maximized.\n",
        "\n",
        "\n",
        "\n",
        "**Kernel Trick**:\n",
        "SVM can handle **non-linearly separable data** using the **kernel trick**.\n",
        "\n",
        "Common kernels:\n",
        "- **Linear Kernel**\n",
        "- **Polynomial Kernel**\n",
        "- **Radial Basis Function (RBF) Kernel**\n",
        "- **Sigmoid Kernel**\n",
        "\n",
        "Kernels transform data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "\n",
        "**Types of SVM**:\n",
        "\n",
        "1. **Linear SVM**  \n",
        "   Used when data is linearly separable.\n",
        "\n",
        "2. **Non-Linear SVM**  \n",
        "   Used when data is not linearly separable (uses kernels).\n",
        "\n",
        "3. **Support Vector Regression (SVR)**  \n",
        "   Used for regression problems.\n",
        "\n",
        "\n",
        "**Advantages of SVM**:\n",
        "1. Works well with high-dimensional data  \n",
        "2. Effective when number of features > number of samples  \n",
        "3. Robust to overfitting  \n",
        "4. Uses only support vectors, making it memory efficient  \n",
        "\n",
        "\n",
        "**Applications of SVM**:\n",
        "- Text classification  \n",
        "- Image recognition  \n",
        "- Bioinformatics  \n",
        "- Face detection  \n",
        "- Spam detection  \n",
        "\n",
        "**Conclusion**:\n",
        "\n",
        "Support Vector Machine is a powerful supervised learning algorithm that constructs an optimal decision boundary by maximizing the margin between classes. With the help of kernel functions, SVM can handle both linear and non-linear problems efficiently, making it a popular choice in real-world machine learning applications.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JCwASdEY7_0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.   What is the Kernel Trick in SVM?\n",
        "\n",
        "--> The **Kernel Trick** is a technique used in **Support Vector Machines (SVM)** to handle **non-linearly separable data**.  \n",
        "It allows SVM to operate in a **higher-dimensional space** without explicitly computing the coordinates of the data in that space.\n",
        "\n",
        "This enables SVM to find a **linear hyperplane in a transformed space**, which corresponds to a **non-linear decision boundary in the original space**.\n",
        "\n",
        "\n",
        "**Kernel Trick is Needed:**\n",
        "- SVM works by finding a **hyperplane** that separates classes.\n",
        "- When data is **linearly separable**, no transformation is needed.\n",
        "- But for **non-linear data**, a simple hyperplane cannot separate the classes.\n",
        "- **Kernel Trick** solves this by mapping data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "**How the Kernel Trick Works**:\n",
        "1. Let $ \\phi(x) $ be a mapping function that transforms input features \\(x\\) into a higher-dimensional space.\n",
        "2. Computing $\\phi(x) $ explicitly is **computationally expensive**, especially in very high dimensions.\n",
        "3. A **kernel function** $K(x_i, x_j)$ computes the **dot product** in the higher-dimensional space directly:\n",
        "$\n",
        "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
        "$\n",
        "\n",
        "4. SVM uses this kernel function in its optimization, avoiding explicit computation of $\\phi(x)$.\n",
        "\n",
        "\n",
        "**Common Kernel Functions**:\n",
        "\n",
        "| Kernel | Formula | Use Case |\n",
        "|--------|---------|---------|\n",
        "| **Linear** | $ K(x_i, x_j) = x_i \\cdot x_j $ | Linearly separable data |\n",
        "| **Polynomial** | $ K(x_i, x_j) = (x_i \\cdot x_j + c)^d $ | Non-linear data with polynomial relationships |\n",
        "| **RBF (Gaussian)** | $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $ | Most common, handles highly non-linear data |\n",
        "| **Sigmoid** | $ K(x_i, x_j) = \\tanh(\\alpha (x_i \\cdot x_j) + c) $ | Similar to neural network activation |\n",
        "\n",
        "\n",
        "\n",
        "**Advantages of Kernel Trick**:\n",
        "1. Can handle **non-linear decision boundaries** efficiently  \n",
        "2. Avoids explicit computation in high-dimensional space (**computationally efficient**)  \n",
        "3. Makes SVM a **powerful and flexible algorithm**  \n",
        "\n",
        "\n",
        "**Disadvantages**:\n",
        "1. Choosing the right **kernel function** and parameters can be tricky  \n",
        "2. Too high-dimensional mappings can lead to **overfitting**  \n",
        "3. Interpretation of results becomes difficult in high dimensions  \n",
        "\n",
        "\n",
        "\n",
        "**Example (Conceptual)**:\n",
        "Suppose we have a 2D dataset with two classes in a circular pattern:\n",
        "\n",
        "- Linear SVM cannot separate them in 2D  \n",
        "- Using **RBF kernel**, SVM maps data to 3D  \n",
        "- In 3D space, a linear hyperplane separates the classes perfectly  \n",
        "- In original 2D space, this corresponds to a **non-linear boundary**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion**:\n",
        "\n",
        "The **Kernel Trick** allows SVM to handle **non-linear classification problems** by mapping data into higher-dimensional spaces implicitly.  \n",
        "It is one of the main reasons why SVM is widely used in **complex real-world datasets** like image recognition, text classification, and bioinformatics.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "lmBOGvaw-7wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "# kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "#Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#load datasets\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "#RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy with Linear Kernel:\", accuracy_score(y_test, y_pred_linear))\n",
        "print(\"Accuracy with RBF Kernel:\", accuracy_score(y_test, y_pred_rbf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0FEuRKRCTXP",
        "outputId": "c9d6f172-ff56-4ce2-ea37-a053108d3a22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9814814814814815\n",
            "Accuracy with RBF Kernel: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "--> The **Naïve Bayes classifier** is a **probabilistic supervised learning algorithm** based on **Bayes' Theorem**.  \n",
        "It is primarily used for **classification problems** and works well with high-dimensional datasets such as **text classification, spam detection, and sentiment analysis**.\n",
        "\n",
        "**Bayes’ Theorem**:\n",
        "Naïve Bayes is based on **Bayes' Theorem**, which calculates the **probability of a class given some features**:\n",
        "\n",
        "$\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $(P(C|X))$ = Posterior probability of class \\(C\\) given features \\(X\\)  \n",
        "- $P(X|C)$ = Likelihood of features \\(X\\) given class \\(C\\)  \n",
        "- $P(C)$ = Prior probability of class \\(C\\)  \n",
        "- $P(X)$ = Probability of features \\(X\\) (normalization factor)\n",
        "\n",
        "\n",
        "\n",
        "**Why It is Called “Naïve”**:\n",
        "It is called **“Naïve”** because it **assumes that all features are independent of each other**, even though in real-world datasets, features are often correlated.\n",
        "\n",
        "- Example: In predicting whether someone will play cricket based on **weather and humidity**, the algorithm assumes weather and humidity are **independent**, which may not be true.  \n",
        "- This simplification makes the algorithm **computationally efficient**.\n",
        "\n",
        "\n",
        "**Working of Naïve Bayes Classifier**:\n",
        "1. Calculate **prior probabilities** \\(P(C)\\) for each class.  \n",
        "2. Calculate **likelihoods** \\(P(X|C)\\) for each feature given class.  \n",
        "3. Use **Bayes’ Theorem** to compute posterior probability \\(P(C|X)\\) for each class.  \n",
        "4. Assign the **class with the highest posterior probability** to the sample.\n",
        "\n",
        "$\n",
        "\\hat{C} = \\arg\\max_{C} P(C) \\prod_{i=1}^{n} P(x_i|C)\n",
        "$\n",
        "\n",
        "Where $x_i$ are the feature values of the instance.\n",
        "\n",
        "**Types of Naïve Bayes Classifier:**\n",
        "1. **Gaussian Naïve Bayes** – Features are continuous and follow a normal distribution.  \n",
        "2. **Multinomial Naïve Bayes** – Features are counts (used in text classification).  \n",
        "3. **Bernoulli Naïve Bayes** – Features are binary (0/1, yes/no).\n",
        "\n",
        "\n",
        "**Advantages**:\n",
        "1. Simple and easy to implement  \n",
        "2. Fast and efficient, even with large datasets  \n",
        "3. Works well with high-dimensional data  \n",
        "4. Requires less training data  \n",
        "5. Robust to irrelevant features  \n",
        "\n",
        "\n",
        "**Applications**:\n",
        "- Spam email detection  \n",
        "- Sentiment analysis  \n",
        "- Document classification  \n",
        "- Medical diagnosis  \n",
        "- Real-time prediction systems  \n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "The Naïve Bayes classifier is a **probabilistic and efficient algorithm** based on Bayes’ Theorem.  \n",
        "It is called **“Naïve”** because it assumes all features are independent. Despite this strong assumption, it performs remarkably well in many real-world applications, especially in **text and high-dimensional datasets**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bsaslpZxEUI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "--->Naïve Bayes has three main variants depending on the type of data and features:\n",
        "\n",
        "## 1. Gaussian Naïve Bayes\n",
        "\n",
        "- **Data Type:** Continuous features  \n",
        "- **Assumption:** Features follow a **normal (Gaussian) distribution**  \n",
        "- **Probability Calculation:** Uses Gaussian probability density function:\n",
        "\n",
        "$\n",
        "P(x_i|C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp \\Big( -\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2} \\Big)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $(x_i)$ = feature value  \n",
        "- $(mu_C)$, $(sigma_C^2)$ = mean and variance of feature $(x_i)$ for class $(C)$\n",
        "\n",
        "- **Use Case:** Predicting numerical continuous data (e.g., height, weight, temperature)  \n",
        "\n",
        "\n",
        "\n",
        "## 2. Multinomial Naïve Bayes\n",
        "\n",
        "- **Data Type:** Discrete features (counts or frequencies)  \n",
        "- **Assumption:** Features represent **counts or occurrences**  \n",
        "- **Probability Calculation:** Uses multinomial distribution:\n",
        "\n",
        "$\n",
        "P(x_i|C) = \\frac{(\\text{count of feature } x_i \\text{ in class } C + 1)}{\\text{total counts in class } C + n}\n",
        "$\n",
        "\n",
        "Where (n) = number of features (Laplace smoothing is applied)  \n",
        "\n",
        "- **Use Case:** Text classification, document classification, spam detection (word frequencies)\n",
        "\n",
        "\n",
        "\n",
        "## 3. Bernoulli Naïve Bayes\n",
        "\n",
        "- **Data Type:** Binary features (0/1, yes/no)  \n",
        "- **Assumption:** Each feature is **Bernoulli-distributed**  \n",
        "- **Probability Calculation:**\n",
        "\n",
        "$\n",
        "P(x_i|C) = p_C^{x_i} (1 - p_C)^{1 - x_i}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $(x_i = 0)$ or $(1)$  \n",
        "- $(p_C)$ = probability of feature $(x_i)$ being 1 given class $(C)$\n",
        "\n",
        "- **Use Case:** Binary occurrence data (e.g., presence/absence of a word in a document)\n",
        "\n",
        "\n",
        "**Comparison Table**:\n",
        "\n",
        "| Feature | Gaussian NB | Multinomial NB | Bernoulli NB |\n",
        "|---------|------------|----------------|--------------|\n",
        "| **Feature Type** | Continuous | Discrete counts | Binary (0/1) |\n",
        "| **Distribution** | Gaussian (Normal) | Multinomial | Bernoulli |\n",
        "| **Use Case** | Numerical prediction | Text classification (word counts) | Text classification (word presence) |\n",
        "| **Probability Formula** | Gaussian PDF | Multinomial probability | Bernoulli probability |\n",
        "| **Example** | Predicting height or weight | Spam email classification using word frequencies | Spam email classification using word presence |\n",
        "\n",
        "**Conclusion**:\n",
        "\n",
        "- **Gaussian NB:** For continuous numerical features  \n",
        "- **Multinomial NB:** For count/frequency data (most common for text)  \n",
        "- **Bernoulli NB:** For binary features (presence/absence data)  \n",
        "\n",
        "Choosing the correct variant depends on **feature type** and **data representation**. Using the wrong variant may reduce accuracy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9hOqvNv8GKkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#Split data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
        "\n",
        "#Gaussian Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train,y_train)\n",
        "\n",
        "#Predict and evaluate\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Guassian Naive Bayes Accuracy\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuhGomFuHcXX",
        "outputId": "3dec033e-b179-4a64-db53-bb5a1dfedc94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guassian Naive Bayes Accuracy 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}