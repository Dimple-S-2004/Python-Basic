{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "1M4QK6tr-DJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "--> Ensemble Learning is a machine learning technique in which multiple individual models (called base learners or weak learners) are trained and combined to solve the same problem, with the goal of improving overall predictive performance compared to any single model.\n",
        "Instead of relying on one model, ensemble learning aggregates the predictions of several models to produce a more accurate, stable, and robust final prediction.\n",
        "\n",
        "**Key Idea Behind Ensemble Learning**:\n",
        "\n",
        "The core idea of ensemble learning is:\n",
        "\n",
        "“A group of diverse models, when combined, performs better than a single model.”\n",
        "\n",
        "This idea is inspired by the principle of collective intelligence, where multiple opinions reduce errors and uncertainty.\n",
        "\n",
        " **Key points of the idea:**\n",
        "\n",
        "* Different models make different errors\n",
        "\n",
        "* Combining them helps cancel out individual mistakes\n",
        "\n",
        "* Reduces overfitting and variance\n",
        "\n",
        "* Improves generalization ability\n",
        "\n",
        "**Why Ensemble Learning Works:** Ensemble learning works effectively because of:\n",
        "\n",
        "(a) Error Reduction\n",
        "\n",
        "Each model has its own bias and variance. By combining models:\n",
        "\n",
        "Random errors are averaged out\n",
        "\n",
        "Prediction becomes more reliable\n",
        "\n",
        "(b) Diversity Among Models\n",
        "\n",
        "Ensembles perform best when base learners are:\n",
        "\n",
        "Trained on different subsets of data\n",
        "\n",
        "Use different algorithms\n",
        "\n",
        "Have different hyperparameters\n",
        "\n",
        "Greater diversity ⇒ Better ensemble performance.\n",
        "\n",
        "**Types of Ensemble Learning Techniques:**\n",
        "\n",
        "* 4.1 Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Models are trained on different random samples of the dataset\n",
        "\n",
        "Final prediction is made by majority voting (classification) or averaging (regression)\n",
        "\n",
        "Example:\n",
        "\n",
        "Random Forest (collection of decision trees)\n",
        "\n",
        "\n",
        "* 4.2 Boosting\n",
        "\n",
        "Models are trained sequentially\n",
        "\n",
        "Each new model focuses more on previously misclassified instances\n",
        "\n",
        "Examples:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "\n",
        "* 4.3 Stacking (Stacked Generalization)\n",
        "\n",
        "Predictions from multiple models are used as inputs to a meta-model\n",
        "\n",
        "Meta-model learns how to best combine base model predictions\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Achieves higher accuracy by learning optimal combinations\n",
        "\n",
        "**Example to Understand Ensemble Learning**:\n",
        "\n",
        "Consider a classification problem where:\n",
        "\n",
        "Model A accuracy = 70%\n",
        "\n",
        "Model B accuracy = 72%\n",
        "\n",
        "Model C accuracy = 68%\n",
        "\n",
        "If these models make different mistakes, combining them using voting can produce an accuracy higher than 72%, showing the strength of ensemble learning.\n",
        "\n",
        "**Conclusion** : Ensemble learning is a powerful machine learning approach that combines multiple models to achieve better accuracy, stability, and generalization. The key idea behind ensemble learning is that diverse models complement each other, and their combined prediction is stronger than any single model. Because of its effectiveness, ensemble learning forms the backbone of many state-of-the-art machine learning systems.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XpHJ0dDI-Hwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between Bagging and Boosting?\n",
        "\n",
        "-->\n",
        "| Feature | Bagging (Bootstrap Aggregating) | Boosting |\n",
        "|--------|--------------------------------|----------|\n",
        "| Basic Idea | Trains multiple models independently on different random samples of data | Trains models sequentially, each correcting the errors of the previous one |\n",
        "| Data Sampling | Uses bootstrap sampling (sampling with replacement) | Uses the same dataset, but changes weights of samples |\n",
        "| Model Dependency | Models are independent of each other | Models are dependent on previous models |\n",
        "| Focus | Reduces variance | Reduces bias |\n",
        "| Handling Misclassified Data | All data points are treated equally | Misclassified data points get higher importance |\n",
        "| Overfitting Control | Very effective in reducing overfitting | Can overfit if data is noisy |\n",
        "| Training Speed | Faster because models can be trained in parallel | Slower due to sequential training |\n",
        "| Robustness to Noise | More robust to noisy data | Sensitive to noisy and outlier data |\n",
        "| Final Prediction | Majority voting (classification) or averaging (regression) | Weighted sum or weighted voting |\n",
        "| Typical Base Learners | High-variance models (e.g., Decision Trees) | Weak learners (e.g., shallow Decision Trees) |\n",
        "| Popular Algorithms | Random Forest | AdaBoost, Gradient Boosting, XGBoost |\n",
        "| Best Used When | Model has high variance | Model has high bias |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3MebZQVPBZWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "--> Bootstrap sampling is a statistical resampling technique in which multiple new datasets are created by randomly sampling from the original dataset with replacement.\n",
        "Each bootstrap sample has the same size as the original dataset, but some observations may appear multiple times, while others may not appear at all.\n",
        "\n",
        "**Key characteristics:**\n",
        "\n",
        "* Sampling is done with replacement\n",
        "\n",
        "* Each sample contains duplicates\n",
        "\n",
        "* Size of each sample = size of original dataset\n",
        "\n",
        "* Used to estimate model variability and improve stability\n",
        "\n",
        "**Bootstrap Sampling is Needed**:\n",
        "\n",
        "In real-world datasets:\n",
        "A single training set may not represent the population well\n",
        "\n",
        "Models trained on one dataset may overfit\n",
        "\n",
        "Predictions can be unstable\n",
        "\n",
        "Bootstrap sampling addresses these problems by creating multiple slightly different datasets, enabling the training of diverse models.\n",
        "\n",
        "**Role of Bootstrap Sampling in Bagging**:\n",
        "\n",
        "Bagging (Bootstrap Aggregating) uses bootstrap sampling as its core mechanism.\n",
        "\n",
        "Process:\n",
        "\n",
        "1. From the original dataset, create multiple bootstrap samples\n",
        "\n",
        "2. Train one model on each bootstrap sample\n",
        "\n",
        "3. Combine predictions of all models using:\n",
        "\n",
        "* Majority voting (classification)\n",
        "\n",
        "* Averaging (regression)\n",
        "\n",
        "Role:\n",
        "\n",
        "* Introduces data diversity\n",
        "\n",
        "* Ensures models are independent\n",
        "\n",
        "* Reduces correlation among models\n",
        "\n",
        "* Improves ensemble performance\n",
        "\n",
        "**Role of Bootstrap Sampling in Random Forest** :\n",
        "\n",
        "Random Forest is an advanced bagging-based ensemble method that uses bootstrap sampling in combination with feature randomness.\n",
        "How Bootstrap Works in Random Forest:\n",
        "\n",
        "a. Each decision tree is trained on a bootstrap sample\n",
        "\n",
        "b. Approximately 63% of original data appears in each sample\n",
        "\n",
        "c. Remaining ~37% data is called Out-of-Bag (OOB) samples\n",
        "\n",
        "**Importance in Random Forest:**\n",
        "\n",
        "* Each tree learns different patterns\n",
        "\n",
        "* Trees become less correlated\n",
        "\n",
        "* Ensemble prediction becomes more accurate and stable\n",
        "\n",
        "**Out-of-Bag (OOB) Error :**\n",
        "\n",
        "* Bootstrap sampling enables OOB error estimation:\n",
        "\n",
        "* Data not selected in a bootstrap sample is used for testing\n",
        "\n",
        "* Provides an unbiased estimate of model performance\n",
        "\n",
        "* Eliminates need for a separate validation set\n",
        "\n",
        "**Benefits of Bootstrap Sampling in Bagging:**\n",
        "\n",
        "* Reduces variance of high-variance models\n",
        "\n",
        "* Prevents overfitting\n",
        "\n",
        "* Improves generalization\n",
        "\n",
        "* Increases robustness\n",
        "\n",
        "* Allows parallel training\n",
        "\n",
        "**Simple Example:** If the dataset has 1,000 samples:\n",
        "Each bootstrap sample also has 1,000 samples Some samples repeat Some are left out (OOB) Each model sees a different view of data\n",
        "\n",
        "**Conclusion** : Bootstrap sampling is a fundamental technique that enables bagging methods like Random Forest to create diverse and independent models. By training each model on a different bootstrap sample, bagging reduces variance, improves stability, and enhances predictive performance. In Random Forest, bootstrap sampling—along with feature randomness—makes the model one of the most powerful and widely used ensemble learning algorithms.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "j_jF2btzCoKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "--> In ensemble learning methods that use bootstrap sampling (such as Bagging and Random Forest), each base model is trained on a bootstrap sample drawn with replacement from the original dataset.\n",
        "\n",
        "**Definition:** Out-of-Bag samples are the data points that are not selected in a particular bootstrap sample and are left out during the training of an individual model.\n",
        "\n",
        "Because of sampling with replacement:\n",
        "\n",
        "Not all training instances are selected\n",
        "\n",
        "About 63% of unique samples appear in each bootstrap sample\n",
        "\n",
        "The remaining ~37% samples are not used in training that model\n",
        "\n",
        "These unused data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "\n",
        "**Why OOB Samples Occur:**\n",
        "\n",
        "* Bootstrap sampling allows repetition\n",
        "\n",
        "* Some data points get selected multiple times\n",
        "\n",
        "* Some data points are never selected\n",
        "\n",
        "Thus, each model naturally has its own mini test set (OOB samples).\n",
        "\n",
        "**What is OOB Score:**\n",
        "The OOB score is a performance evaluation metric that uses OOB samples to assess the ensemble model’s accuracy without using a separate validation set.\n",
        "\n",
        "**Definition**: OOB score is the average prediction accuracy of the ensemble model computed using out-of-bag samples.\n",
        "\n",
        "**How OOB Score is Calculated**\n",
        "\n",
        "Step-by-step process:\n",
        "\n",
        "* For each data point, identify the trees where it was not included in training\n",
        "\n",
        "* Use those trees to make predictions for that data point\n",
        "\n",
        "* Combine predictions (voting or averaging)\n",
        "\n",
        "* Compare predictions with true labels\n",
        "\n",
        "* Compute overall accuracy (classification) or error (regression)\n",
        "\n",
        "This process is repeated for all data points.\n",
        "\n",
        "**Simple Example**\n",
        "\n",
        "* If a dataset has 1,000 samples:\n",
        "\n",
        "* Each tree trains on ~630 samples\n",
        "\n",
        "* ~370 samples are OOB\n",
        "\n",
        "* Those 370 samples evaluate that tree\n",
        "\n",
        "* Final OOB score combines results from all trees\n",
        "\n",
        "**Role of OOB Score in Ensemble Models:**\n",
        "\n",
        "(a) Model Evaluation\n",
        "\n",
        "* Provides an unbiased estimate of model performance\n",
        "\n",
        "* Comparable to cross-validation results\n",
        "\n",
        "(b) No Need for Validation Set\n",
        "\n",
        "* Saves data\n",
        "\n",
        "* Uses entire dataset for training and evaluation\n",
        "\n",
        "(c) Prevents Overfitting\n",
        "\n",
        "* Evaluates performance on unseen data\n",
        "\n",
        "* Helps detect generalization errors\n",
        "\n",
        "**Conclusion** : Out-of-Bag samples are a natural by-product of bootstrap sampling used in bagging-based ensemble models like Random Forest. The OOB score provides an efficient, unbiased, and reliable method to evaluate ensemble performance without the need for a separate validation dataset, making it a powerful tool for model assessment.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "EEZIdTiFFCh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "--> Feature importance refers to techniques used to determine how much each input feature contributes to a model’s predictions. Both Decision Trees and Random Forests provide built-in feature importance measures, but they differ significantly in reliability, stability, and interpretability.\n",
        "\n",
        "Difference between Decision Tree VS Random Forest :\n",
        "\n",
        "| Feature | Decision Tree | Random Forest |\n",
        "|--------|---------------|---------------|\n",
        "| Model Type | Single model | Ensemble of trees |\n",
        "| Feature Importance Basis | Impurity reduction in one tree | Averaged impurity reduction across trees |\n",
        "| Stability | Low (high variance) | High (low variance) |\n",
        "| Sensitivity to Data Changes | Very sensitive | Less sensitive |\n",
        "| Bias Toward High-Cardinality Features | High | Reduced but still present |\n",
        "| Interpretability | Very high | Moderate |\n",
        "| Robustness | Low | High |\n",
        "| Overfitting Risk | High | Low |\n",
        "| Handling Feature Interactions | Limited | Strong |\n",
        "| Preferred Use | Simple, explainable models | Accurate, production-level models |\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If a dataset contains features Age, Salary, and Experience:\n",
        "\n",
        "* A single decision tree might show Salary as most important due to one strong split\n",
        "\n",
        "* A random forest may rank Experience higher after averaging across hundreds of trees\n",
        "\n",
        "This shows how Random Forest gives more reliable feature importance.\n",
        "\n",
        "**Conclusion:** Feature importance in a single decision tree is simple and interpretable but unstable and prone to bias. In contrast, Random Forest provides more reliable and robust feature importance by averaging contributions across multiple trees. Therefore, Decision Trees are suitable for explanation, while Random Forests are preferred for accurate and dependable feature importance analysis in real-world applications.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9u2mzPg-HET1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "#train random forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "#Feature importance\n",
        "importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "top_5 = importance.sort_values(ascending=False).head(5)\n",
        "print(top_5)\n",
        ""
      ],
      "metadata": {
        "id": "Pz8u4rydI8j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368e5b44-2144-468f-ffef-5278d72db92f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "#Spliting data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "#Bagging Classifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvauEG8h38IH",
        "outputId": "f8e2571c-34d6-4aff-e141-48af59803957"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92GwSB14AgM",
        "outputId": "19bae88b-98df-41e1-f0fa-ff06e1aaf5ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
        "                        n_estimators=50,\n",
        "                        random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(\"Bagging MSE:\", bag_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98-Pd1lr4E1e",
        "outputId": "d0991ec4-72cf-4ba2-fed8-bf0a068000db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.25787382250585034\n",
            "Random Forest MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "-->Predicting Loan Default Using Ensemble Learning:\n",
        "\n",
        "**Introduction**:\n",
        "As a data scientist in a financial institution, predicting whether a customer will default on a loan is very important. A wrong decision can cause financial loss or reject a genuine customer. Since we have customer demographic details like age, income, job type, credit score, and transaction history such as spending and repayment behavior, ensemble learning methods are used to build a more accurate and reliable model.\n",
        "\n",
        "**Step 1: Choosing Between Bagging and Boosting**:The choice between bagging and boosting depends on:\n",
        "* How complex and noisy the data is\n",
        "* Whether the model suffers from high bias or high variance\n",
        "* The business risk involved in wrong predictions\n",
        "**Preferred Choice: Boosting**\n",
        "Loan default data usually has complex patterns and class imbalance. Single models often underperform. Boosting is useful because it focuses more on customers that are hard to classify, especially high-risk defaulters.\n",
        "**Why Boosting is better here:**\n",
        "* It improves weak models step by step\n",
        "* Gives more importance to customers likely to default\n",
        "* Algorithms like Gradient Boosting and XGBoost are widely used in banking\n",
        "Random Forest (bagging) can be used as a baseline model, but boosting is better suited for final deployment.\n",
        "\n",
        "**Step 2: Handling Overfitting:** Overfitting is risky in finance because the model may perform well on training data but fail on new customers.\n",
        "**Steps to control overfitting:**\n",
        "* Use cross-validation to check consistency\n",
        "* Apply regularization techniques\n",
        "   a.Limit tree depth\n",
        "   b.Use a small learning rate in boosting\n",
        "* Use early stopping to prevent over-training\n",
        "* Remove irrelevant or highly correlated features\n",
        "* Handle class imbalance using class weights or SMOTE\n",
        "Ensemble methods naturally reduce overfitting by combining multiple models.\n",
        "\n",
        "**Step 3: Selecting Base Models** : Chosen Base Model: Decision Trees (shallow trees)\n",
        "**Reasons:**\n",
        "* Capture non-linear relationships\n",
        "* Handle both numerical and categorical data\n",
        "* Robust to outliers\n",
        "* Easy to interpret, which is important in finance\n",
        "\n",
        "**Examples of Ensembles:**\n",
        "* Random Forest → many independent trees\n",
        "* Gradient Boosting / XGBoost → trees built sequentially to correct errors\n",
        "\n",
        "Shallow decision trees act as weak learners, making them ideal for ensemble learning.\n",
        "\n",
        "**Step 4: Evaluating Performance Using Cross-Validation** : A single train-test split is not reliable for financial data.\n",
        "\n",
        "**Cross-Validation Approach:**\n",
        "* Use k-fold cross-validation (k = 5 or 10)\n",
        "* Each fold acts as validation once\n",
        "* Gives stable and trustworthy results\n",
        "\n",
        "**Evaluation Metrics Used:**\n",
        "* ROC-AUC → measures overall model performance\n",
        "* Precision and Recall → important for default cases\n",
        "* F1-score → balances false positives and false negatives\n",
        "* Confusion Matrix → helps understand business impact\n",
        "Random Forest can also use the Out-of-Bag (OOB) score for internal validation.\n",
        "\n",
        "**Step 5: How Ensemble Learning Improves Decision-Making**\n",
        "** Benefits in Real-World Finance:**\n",
        "1. Higher Accuracy\n",
        "* Combines multiple models\n",
        "* Reduces individual model errors\n",
        "\n",
        "2. Better Risk Detection\n",
        "* Boosting focuses on high-risk customers\n",
        "* Reduces chances of missing defaulters\n",
        "3. Stable and Reliable Predictions\n",
        "* Less affected by noise\n",
        "* Safer for real-world use\n",
        "4. Fair and Consistent Loan Decisions\n",
        "* Avoids bias of a single model\n",
        "* Ensures consistent lending rules\n",
        "5. Regulatory and Business Confidence\n",
        "* Feature importance explains decisions\n",
        "* Helps justify loan approval or rejection\n",
        "\n",
        "**Conclusion** : To predict loan default effectively, ensemble learning is a strong and reliable approach. By using boosting to reduce bias, controlling overfitting through regularization and cross-validation, selecting decision trees as base models, and evaluating performance using proper metrics, ensemble models significantly improve prediction quality. In a financial environment, this leads to better risk management, higher accuracy, and more responsible lending decisions."
      ],
      "metadata": {
        "id": "LBi-TDG2BygH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE:\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a sample loan default dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    n_redundant=2,\n",
        "    weights=[0.7, 0.3],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest (Bagging)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Gradient Boosting (Boosting)\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_pred))\n",
        "\n",
        "print(\"Random Forest ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
        "print(\"Gradient Boosting ROC-AUC:\", roc_auc_score(y_test, gb.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Cross-validation for Boosting\n",
        "cv_score = cross_val_score(gb, X, y, cv=5, scoring='roc_auc')\n",
        "print(\"Cross-Validated ROC-AUC:\", cv_score.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I32YiC4ZI_rs",
        "outputId": "d270b2d7-b982-48f2-f127-1d9adc1c4563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8933333333333333\n",
            "Gradient Boosting Accuracy: 0.87\n",
            "Random Forest ROC-AUC: 0.9446699362985512\n",
            "Gradient Boosting ROC-AUC: 0.940582896442866\n",
            "Cross-Validated ROC-AUC: 0.957935234950213\n"
          ]
        }
      ]
    }
  ]
}