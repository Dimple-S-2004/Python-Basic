{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques\n"
      ],
      "metadata": {
        "id": "OHW0ipG2JHQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "--> Boosting is an ensemble learning technique in machine learning that combines multiple weak learners to create a strong learner with high predictive accuracy. A weak learner is a model that performs only slightly better than random guessing. Boosting improves performance by training models sequentially, where each new model focuses on correcting the errors made by the previous models.The main objective of boosting is to reduce bias and variance and improve overall model accuracy.\n",
        "\n",
        "\n",
        "\n",
        " **Weak Learners**:A weak learner is a simple model that:\n",
        "- Has low complexity\n",
        "- Performs slightly better than random guessing\n",
        "- Examples include decision stumps (single-level decision trees) and simple linear models\n",
        "\n",
        "Although weak learners perform poorly individually, boosting combines them effectively to achieve strong performance.\n",
        "\n",
        "\n",
        "\n",
        "**Working of Boosting (Step-by-Step)**:\n",
        "1. Assign equal weights to all training samples.\n",
        "2. Train the first weak learner on the dataset.\n",
        "3. Identify misclassified samples.\n",
        "4. Increase the weights of misclassified samples.\n",
        "5. Train the next weak learner by giving more importance to difficult samples.\n",
        "6. Repeat the process for a fixed number of iterations.\n",
        "7. Combine all weak learners using weighted voting (classification) or weighted averaging (regression).\n",
        "\n",
        "\n",
        "**Popular Boosting Algorithms:**\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting):\n",
        "- Adjusts sample weights based on classification errors.\n",
        "- Misclassified samples receive higher weights.\n",
        "- Learners with lower error rates receive higher importance.\n",
        "\n",
        "Weight calculation formula:\n",
        "$\n",
        "\\alpha = \\frac{1}{2} \\ln\\left(\\frac{1 - error}{error}\\right)\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "2. Gradient Boosting:\n",
        "- Builds models sequentially to minimize a loss function.\n",
        "- Each new model learns the residual errors of the previous model.\n",
        "- Uses gradient descent optimization.\n",
        "\n",
        "\n",
        "\n",
        "3. XGBoost (Extreme Gradient Boosting):\n",
        "- An optimized version of Gradient Boosting.\n",
        "- Includes regularization, tree pruning, and parallel processing.\n",
        "- Highly efficient and widely used in real-world applications.\n",
        "\n",
        "\n",
        "\n",
        "**How Boosting Improves Weak Learners:**\n",
        "\n",
        "- Focuses more on difficult and misclassified samples\n",
        "- Corrects errors sequentially\n",
        "- Assigns higher weight to better-performing learners\n",
        "- Reduces bias and underfitting\n",
        "- Improves generalization on unseen data\n",
        "\n",
        "\n",
        "\n",
        "**Advantages of Boosting:**\n",
        "- Converts weak learners into strong learners\n",
        "- High prediction accuracy\n",
        "- Reduces bias and variance\n",
        "- Works well with simple models\n",
        "- Effective on complex datasets\n",
        "\n",
        "\n",
        "\n",
        "**Limitations of Boosting:**\n",
        "- Sensitive to noisy data and outliers\n",
        "- Computationally expensive\n",
        "- Risk of overfitting if over-trained\n",
        "- Less interpretable than single models\n",
        "\n",
        "\n",
        "**Applications of Boosting:**\n",
        "- Loan default prediction\n",
        "- Fraud detection\n",
        "- Medical diagnosis\n",
        "- Spam filtering\n",
        "- Stock market prediction\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion** : Boosting is a powerful ensemble technique that improves weak learners by training them sequentially and focusing on correcting previous mistakes. By combining multiple simple models, boosting produces a strong learner with high accuracy. Algorithms like AdaBoost, Gradient Boosting, and XGBoost have made boosting one of the most effective techniques in modern machine learning.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sQ8ACIDdJUCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "--> Difference between AdaBoost and Gradient Boosting (Training Perspective)\n",
        "AdaBoost and Gradient Boosting are both boosting techniques, but they differ\n",
        "fundamentally in how models are trained and how errors are handled.\n",
        "\n",
        "1. Training Approach\n",
        "- **AdaBoost** trains models sequentially by **reweighting training samples**.\n",
        "  Misclassified samples receive higher weights so that the next model focuses\n",
        "  more on difficult instances.\n",
        "- **Gradient Boosting** trains models sequentially by **fitting the residual\n",
        "  errors** (negative gradients) of the previous model.\n",
        "\n",
        "\n",
        "2. Error Handling\n",
        "- **AdaBoost** increases the importance of misclassified points directly by\n",
        "  updating sample weights.\n",
        "- **Gradient Boosting** minimizes a **loss function** using gradient descent and\n",
        "  does not explicitly change sample weights.\n",
        "\n",
        "\n",
        "\n",
        "3. Objective Function\n",
        "- **AdaBoost** does not optimize a general loss function; it focuses on\n",
        "  classification error.\n",
        "- **Gradient Boosting** explicitly optimizes a chosen loss function (e.g., MSE,\n",
        "  log loss).\n",
        "\n",
        "\n",
        "\n",
        "4. Model Contribution\n",
        "- **AdaBoost** assigns a **weight to each weak learner** based on its accuracy.\n",
        "- **Gradient Boosting** adds each new learner scaled by a **learning rate**.\n",
        "\n",
        "\n",
        "\n",
        " 5. Sensitivity to Noise\n",
        "- **AdaBoost** is more sensitive to noisy data and outliers due to increasing\n",
        "  weights of misclassified samples.\n",
        "- **Gradient Boosting** is more robust as it optimizes a smooth loss function.\n",
        "\n",
        "\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Aspect | AdaBoost | Gradient Boosting |\n",
        "|------|---------|------------------|\n",
        "| Training Focus | Reweight samples | Fit residuals |\n",
        "| Error Handling | Sample weight update | Loss minimization |\n",
        "| Loss Function | Implicit | Explicit |\n",
        "| Robustness | Less robust to noise | More robust |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JWY5-Ex2P1C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  How does regularization help in XGBoost?\n",
        "\n",
        "--> Regularization in XGBoost is used to **prevent overfitting** and improve the\n",
        "model’s ability to generalize to unseen data. Unlike traditional boosting\n",
        "algorithms, XGBoost explicitly includes regularization terms in its objective\n",
        "function to control model complexity.\n",
        "\n",
        "\n",
        "**Regularized Objective Function in XGBoost:**\n",
        "\n",
        "The objective function in XGBoost is defined as:\n",
        "\n",
        "$\n",
        "\\text{Objective} = \\text{Loss Function} + \\text{Regularization Term}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- The **loss function** measures prediction error.\n",
        "- The **regularization term** penalizes complex trees.\n",
        "\n",
        "\n",
        "\n",
        "**Types of Regularization in XGBoost:**\n",
        "\n",
        "### 1. L1 Regularization (Lasso)\n",
        "- Controlled by parameter **alpha (α)**.\n",
        "- Encourages sparsity by shrinking some leaf weights to zero.\n",
        "- Helps in feature selection.\n",
        "\n",
        "\n",
        "\n",
        "### 2. L2 Regularization (Ridge)\n",
        "- Controlled by parameter **lambda (λ)**.\n",
        "- Penalizes large leaf weights.\n",
        "- Prevents overly complex trees.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Tree Complexity Regularization\n",
        "- Penalizes the **number of leaf nodes** in a tree.\n",
        "- Controlled by parameter **gamma (γ)**.\n",
        "- A split is made only if it improves the model by at least γ.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Depth Control\n",
        "- Parameters such as **max_depth** and **min_child_weight** limit tree growth.\n",
        "- Prevents the model from learning overly specific patterns.\n",
        "\n",
        "\n",
        "\n",
        "**Benefits of Regularization in XGBoost:**\n",
        "\n",
        "- Reduces overfitting\n",
        "- Improves model generalization\n",
        "- Controls model complexity\n",
        "- Enhances stability on noisy datasets\n",
        "- Produces simpler and more interpretable models\n",
        "\n",
        "\n",
        "\n",
        "**Regularization is Important in XGBoost:**\n",
        "Boosting algorithms are prone to overfitting due to sequential learning.\n",
        "XGBoost’s built-in regularization ensures that each new tree adds useful\n",
        "information without memorizing noise, making it more robust than traditional\n",
        "boosting methods.\n",
        "\n",
        "\n",
        "**Conclusion:**Regularization in XGBoost plays a crucial role in controlling tree complexity,penalizing large weights, and preventing overfitting. By combining L1, L2, and\n",
        "tree-based regularization techniques, XGBoost achieves high accuracy while\n",
        "maintaining strong generalization performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GNRTXvF2QgKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "-->CatBoost (Categorical Boosting) is a gradient boosting algorithm specifically\n",
        "designed to handle categorical features efficiently without extensive manual\n",
        "preprocessing. Unlike traditional boosting methods, CatBoost processes\n",
        "categorical data directly and reduces common issues such as overfitting and\n",
        "target leakage.\n",
        "\n",
        "\n",
        "\n",
        "##Key Reasons for CatBoost’s Efficiency\n",
        "\n",
        "**1. Native Handling of Categorical Features:**\n",
        "- CatBoost can work directly with categorical variables.\n",
        "- No need for one-hot encoding or label encoding.\n",
        "- Reduces dimensionality and memory usage.\n",
        "\n",
        "\n",
        "**2. Ordered Target Encoding:**\n",
        "- CatBoost uses **ordered target statistics** instead of standard target\n",
        "  encoding.\n",
        "- Prevents **target leakage** by calculating statistics using only past data.\n",
        "- Improves generalization and model stability.\n",
        "\n",
        "\n",
        "\n",
        "**3. Reduction of Overfitting:**\n",
        "- Uses permutation-driven training to ensure unbiased encoding.\n",
        "- Combines encoding and training in a way that avoids memorizing noise.\n",
        "\n",
        "\n",
        "**4. Efficient Handling of High-Cardinality Features:**\n",
        "- Works well with features having a large number of categories.\n",
        "- Avoids feature explosion caused by one-hot encoding.\n",
        "\n",
        "\n",
        "\n",
        "**5. Symmetric (Oblivious) Decision Trees:**\n",
        "- Uses symmetric trees where the same split is applied at each level.\n",
        "- Faster training and prediction.\n",
        "- More robust and easier to interpret.\n",
        "\n",
        "\n",
        "\n",
        "**6. Minimal Preprocessing Required:**\n",
        "- Automatically handles missing values and categorical variables.\n",
        "- Saves time and reduces preprocessing complexity.\n",
        "\n",
        "\n",
        "\n",
        " **Advantages of CatBoost for Categorical Data:**\n",
        "- No manual encoding required\n",
        "- Lower risk of target leakage\n",
        "- Better generalization performance\n",
        "- Faster training with large categorical datasets\n",
        "- Suitable for real-world datasets with mixed feature types\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:** CatBoost is considered efficient for handling categorical data because it\n",
        "natively processes categorical features using ordered target encoding and\n",
        "permutation-based learning. By preventing target leakage and reducing\n",
        "dimensionality, CatBoost delivers high accuracy, faster training, and better\n",
        "generalization compared to traditional boosting algorithms.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "f_7_QXjRROy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "--> Boosting and bagging are both ensemble learning techniques, but boosting is\n",
        "preferred in applications where **reducing bias, handling complex patterns,\n",
        "and improving prediction accuracy** are critical. Since boosting trains models\n",
        "sequentially and focuses on correcting errors, it is more suitable for many\n",
        "real-world problems.\n",
        "\n",
        "## Key Applications of Boosting\n",
        "\n",
        "### 1. Credit Risk and Loan Default Prediction\n",
        "- Financial institutions use boosting to detect subtle patterns in customer\n",
        "  behavior.\n",
        "- Boosting models like XGBoost improve accuracy by focusing on hard-to-classify\n",
        "  defaulters.\n",
        "- Bagging may miss complex relationships due to independent training.\n",
        "\n",
        "\n",
        "### 2. Fraud Detection\n",
        "- Fraud cases are rare and difficult to classify.\n",
        "- Boosting emphasizes misclassified minority class samples.\n",
        "- Helps in detecting complex and evolving fraud patterns.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Medical Diagnosis\n",
        "- Used in disease prediction (e.g., cancer detection).\n",
        "- Boosting captures nonlinear interactions between symptoms and test results.\n",
        "- High accuracy is critical; boosting reduces bias.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Spam and Email Filtering\n",
        "- Boosting improves classification by learning from previously misclassified\n",
        "  emails.\n",
        "- Handles subtle differences between spam and legitimate emails.\n",
        "- AdaBoost and Gradient Boosting are commonly used.\n",
        "\n",
        "\n",
        "\n",
        "### 5. Customer Churn Prediction\n",
        "- Telecom and subscription-based companies use boosting to identify customers\n",
        "  likely to leave.\n",
        "- Boosting focuses on customers that are difficult to predict.\n",
        "- Produces better recall for at-risk customers.\n",
        "\n",
        "\n",
        "\n",
        "### 6. Search Engine Ranking and Recommendation Systems\n",
        "- Boosting algorithms such as Gradient Boosting and XGBoost are widely used.\n",
        "- Capture complex feature interactions.\n",
        "- Improve ranking quality and personalization.\n",
        "\n",
        "\n",
        "\n",
        "### 7. Stock Market and Financial Forecasting\n",
        "- Boosting models capture nonlinear trends in financial data.\n",
        "- Performs better than bagging in complex, noisy environments.\n",
        "\n",
        "\n",
        "\n",
        "**Why Boosting is Preferred Over Bagging in These Applications:**\n",
        "\n",
        "- Better bias reduction\n",
        "- Learns complex patterns\n",
        "- Focuses on hard-to-predict samples\n",
        "- Higher predictive accuracy\n",
        "- More effective for structured/tabular data\n",
        "\n",
        "\n",
        "**Conclusion:** Boosting techniques are preferred over bagging methods in real-world\n",
        "applications where **accuracy, bias reduction, and learning complex patterns**\n",
        "are essential. Domains such as finance, healthcare, fraud detection, and\n",
        "recommendation systems benefit significantly from boosting due to its\n",
        "sequential learning and error-correction mechanism.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "M8d2AxlCSFVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to:\n",
        "# ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "# ● Print the model accuracy\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "ada_model = AdaBoostClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCjM5Sd9St7z",
        "outputId": "96e8b6ff-e6d1-4d45-c2e9-b59336224c85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# ● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# ● Evaluate performance using R-squared score\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpzn7XpGTsop",
        "outputId": "1cde1769-ec27-4554-b17c-e9316423db2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# ● Tune the learning rate using GridSearchCV\n",
        "# ● Print the best parameters and accuracy\n",
        "\n",
        "# Install XGBoost if needed\n",
        "!pip install xgboost\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier **without use_label_encoder**\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',   # Binary classification\n",
        "    eval_metric='logloss',          # Evaluation metric\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy and best params\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3TBjqoPUB7Z",
        "outputId": "d3613018-5294-45d3-a204-cff6447ecd29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.28.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Write a Python program to:\n",
        "# ● Train a CatBoost Classifier\n",
        "# ● Plot the confusion matrix using seaborn\n",
        "\n",
        "# Install CatBoost if not already installed\n",
        "!pip install catboost\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# CatBoost model\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "6EF9RluMUxbm",
        "outputId": "067fd794-245c-400a-9b58-c18d12e0217a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "--> Loan Default Prediction using Boosting Techniques :We are tasked with predicting loan default in a FinTech setting. The dataset is **imbalanced**, contains **missing values**, and has **numeric and categorical features**. Boosting techniques are ideal due to their ability to reduce bias, handle complex feature interactions, and improve performance on structured/tabular data.\n",
        "\n",
        "\n",
        "## Step-by-Step Data Science Pipeline\n",
        "\n",
        "### 1. Data Preprocessing\n",
        "- **Handling Missing Values**\n",
        "  - Numeric features → Fill with mean/median.\n",
        "  - Categorical features → Fill with mode or \"Unknown\".\n",
        "- **Encoding Categorical Features**\n",
        "  - Use **Label Encoding** or **CatBoost’s native handling** if using CatBoost.\n",
        "- **Scaling**\n",
        "  - Optional for boosting algorithms; tree-based methods are mostly scale-invariant.\n",
        "- **Handling Imbalance**\n",
        "  - Techniques like **SMOTE**, **Random Oversampling**, or using `scale_pos_weight` in XGBoost.\n",
        "\n",
        "\n",
        "### 2. Choice of Boosting Algorithm\n",
        "- **AdaBoost:** Simple, but sensitive to noisy data and outliers.  \n",
        "- **XGBoost:** Efficient and widely used; can handle imbalance via `scale_pos_weight`.  \n",
        "- **CatBoost:** Best for categorical features; handles missing values and categorical features natively.  \n",
        "\n",
        "**Choice:** **CatBoostClassifier**, due to:\n",
        "- Native categorical handling\n",
        "- Robust to missing values\n",
        "- Performs well on imbalanced tabular datasets\n",
        "\n",
        "\n",
        "\n",
        "### 3. Hyperparameter Tuning Strategy\n",
        "- Use **GridSearchCV** or **RandomizedSearchCV** for:\n",
        "  - `learning_rate`\n",
        "  - `depth`\n",
        "  - `iterations`\n",
        "  - `l2_leaf_reg`\n",
        "- Can use **early stopping** to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Evaluation Metrics\n",
        "- **Accuracy:** Baseline, but not enough for imbalanced datasets.\n",
        "- **Precision & Recall:** Important to minimize false negatives (loans predicted safe but default).\n",
        "- **F1-Score:** Balance between precision and recall.\n",
        "- **ROC-AUC Score:** Measures model’s discriminative power.\n",
        "\n",
        "\n",
        "\n",
        "### 5. Business Benefits\n",
        "- Accurate prediction reduces financial losses from defaults.\n",
        "- Allows targeted interventions for at-risk customers.\n",
        "- Improves risk-based pricing and decision-making.\n",
        "- Supports compliance and responsible lending.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "voFTfsEzU8bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Python Implementation (Example)\n",
        "# Install necessary libraries (uncomment if needed)\n",
        "# !pip install catboost imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "# Example: Replace this with your actual loan dataset\n",
        "# df = pd.read_csv('loan_data.csv')\n",
        "\n",
        "# For demonstration, we'll create a synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X_dummy, y_dummy = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    n_redundant=2,\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.8, 0.2],  # Imbalanced\n",
        "    flip_y=0.05,\n",
        "    random_state=42\n",
        ")\n",
        "df = pd.DataFrame(X_dummy, columns=[f'feature_{i}' for i in range(10)])\n",
        "df['default'] = y_dummy\n",
        "\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "# Introduce missing values for demonstration\n",
        "df.loc[0:10, 'feature_0'] = np.nan\n",
        "df.fillna(df.median(), inplace=True)  # Fill numeric NaNs\n",
        "\n",
        "# Step 3: Split Features and Target\n",
        "X = df.drop('default', axis=1)\n",
        "y = df['default']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 4: Handle Imbalance using SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 5: Initialize CatBoostClassifier\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    eval_metric='AUC',\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Step 6: Make Predictions and Evaluate\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Accuracy and ROC-AUC\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Step 7: Print Results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxdq4Bn0XjsY",
        "outputId": "3eadc4f4-caa4-4d41-da7a-63fb86bebfc8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.1}\n",
            "Test Accuracy: 0.925\n",
            "ROC-AUC Score: 0.9437914406268837\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95       158\n",
            "           1       0.86      0.76      0.81        42\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.90      0.87      0.88       200\n",
            "weighted avg       0.92      0.93      0.92       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion** :\n",
        "- **Explanation:**  The model handles **imbalanced classes** using SMOTE.  \n",
        "- **CatBoost** natively handles categorical features (if present) and missing values.  \n",
        "- **Hyperparameter tuning** improves predictive performance.  \n",
        "- Business benefits include **reducing defaults**, improving **risk assessment**, and supporting **data-driven decisions**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4YhHOKCwYFKD"
      }
    }
  ]
}