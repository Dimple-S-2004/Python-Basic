{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA\n"
      ],
      "metadata": {
        "id": "dWoff9T6rkrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "--> K-Nearest Neighbors (KNN) is a supervised, non-parametric, and instance-based machine learning algorithm used for both classification and regression problems.\n",
        "It is called a lazy learning algorithm because it does not build an explicit model during training; instead, it stores the training data and performs computation only at prediction time.\n",
        "\n",
        "**How KNN Works:** The core idea of KNN is that similar data points exist close to each other in the feature space.\n",
        "\n",
        "**General Steps**\n",
        "\n",
        "- Choose the number of neighbors K.\n",
        "\n",
        "- Calculate the distance between the new data point and all training points\n",
        "(commonly Euclidean distance).\n",
        "\n",
        "- Select the K nearest neighbors.\n",
        "\n",
        "- Aggregate the neighbors’ outputs to make a prediction.\n",
        "\n",
        "**Distance Measures:**\n",
        "Commonly used distance metrics include:\n",
        "\n",
        "- Euclidean distance\n",
        "\n",
        "- Manhattan distance\n",
        "\n",
        "- Minkowski distance\n",
        "\n",
        "- Cosine similarity\n",
        "\n",
        "**KNN for Classification**\n",
        "\n",
        "In classification problems:\n",
        "\n",
        "- The algorithm finds the K nearest neighbors.\n",
        "\n",
        "- The class label is assigned using majority voting among the neighbors.\n",
        "\n",
        "Example:\n",
        "\n",
        "If K = 5 and among the nearest neighbors:\n",
        "\n",
        "* 3 belong to class “Yes\"\n",
        "\n",
        "- 2 belong to class “No”\n",
        "\n",
        "Then the predicted class is “Yes”.\n",
        "\n",
        "**KNN for Regression**\n",
        "\n",
        "In regression problems:\n",
        "\n",
        "- The algorithm finds the K nearest neighbors.\n",
        "\n",
        "- The output value is the average (or weighted average) of the neighbor's target values.\n",
        "\n",
        "Example:\n",
        "\n",
        "If K = 3 and the neighbors’ values are:\n",
        "\n",
        "50, 60, 70\n",
        "\n",
        "The predicted value is: 60\n",
        "\n",
        "$\n",
        "50 + 60 + 70 / 3\n",
        "$\n",
        "\n",
        "\n",
        "**Advantages of KNN:**\n",
        "\n",
        "- Simple and easy to understand\n",
        "\n",
        "- No training phase\n",
        "\n",
        "- Works well with small datasets\n",
        "\n",
        "- Can model complex decision boundaries\n",
        "\n",
        "\n",
        "**Conclusion** : K-Nearest Neighbors (KNN) is a simple yet powerful algorithm that makes predictions based on the similarity between data points. In classification, it uses majority voting, while in regression, it predicts values by averaging neighbors’ outputs. Despite its simplicity, proper choice of K and distance metrics is crucial for optimal performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nuld1VtUrrxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "--> The Curse of Dimensionality refers to the set of problems that arise when the number of features (dimensions) increases in a dataset. As dimensionality grows, the data space becomes increasingly sparse, making it difficult for distance-based algorithms to find meaningful patterns.\n",
        "\n",
        "**the Curse of Dimensionality Occurs:**\n",
        "- In high-dimensional space, data points become far apart from each other.\n",
        "\n",
        "- The volume of the feature space grows exponentially with dimensions.\n",
        "\n",
        "- A much larger amount of data is required to maintain the same data density.\n",
        "\n",
        "**Effect of Curse of Dimensionality on KNN:** KNN relies entirely on distance calculations, which are strongly affected by high dimensionality.\n",
        "\n",
        "1. Distance Becomes Less Meaningful :\n",
        "In high dimensions, the distances between nearest and farthest neighbors become very similar.This reduces the ability of KNN to distinguish between close and distant points.\n",
        "\n",
        "2. Loss of Neighborhood Concept:KNN assumes nearby points are similar.\n",
        "In high dimensions, all points appear almost equally distant, breaking this assumption.\n",
        "\n",
        "3. Increased Computational Cost: More features increase distance calculation time.\n",
        "Prediction becomes slow and inefficient.\n",
        "\n",
        "4. Higher Risk of Overfitting : Noise increases with more features.\n",
        "KNN may rely on irrelevant features, reducing generalization.\n",
        "\n",
        "**Impact on KNN Performance:**\n",
        "\n",
        "- Decreased accuracy\n",
        "\n",
        "- Poor generalization\n",
        "\n",
        "- Slower predictions\n",
        "\n",
        "- Increased sensitivity to noise\n",
        "\n",
        "**How to Mitigate the Curse of Dimensionality in KNN:**\n",
        "\n",
        "- Feature selection (remove irrelevant features)\n",
        "\n",
        "- Dimensionality reduction (PCA, LDA)\n",
        "\n",
        "- Feature scaling and normalization\n",
        "\n",
        "- Use distance-weighted KNN\n",
        "\n",
        "- Increase dataset size (if possible)\n",
        "\n",
        "**Conclusion** : The Curse of Dimensionality significantly degrades KNN performance by making distance measures unreliable in high-dimensional spaces. Since KNN depends on the notion of proximity, increased dimensionality leads to reduced accuracy, higher computational cost, and poor generalization. Applying dimensionality reduction and feature selection techniques is essential for effective KNN performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PqMOlh5pt_6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "--> Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving as much variance (information) as possible.\n",
        "\n",
        "PCA creates new variables called principal components, which are linear combinations of the original features and are mutually uncorrelated.\n",
        "\n",
        "**How PCA Works:**\n",
        "\n",
        "1. Standardize the data.\n",
        "\n",
        "2. Compute the covariance matrix.\n",
        "\n",
        "3. Calculate eigenvalues and eigenvectors.\n",
        "\n",
        "4. Sort eigenvectors by decreasing eigenvalues.\n",
        "\n",
        "5. Select the top k principal components.\n",
        "\n",
        "6. Project the original data onto these components.\n",
        "\n",
        "The first principal component captures the maximum variance, the second captures the next highest variance, and so on.\n",
        "\n",
        "**Key Characteristics of PCA:**\n",
        "\n",
        "- Unsupervised technique\n",
        "\n",
        "- Reduces dimensionality\n",
        "\n",
        "- Removes multicollinearity\n",
        "\n",
        "- Produces new transformed features\n",
        "\n",
        "- Commonly used for visualization and noise reduction\n",
        "\n",
        "**Feature Selection** : Feature selection is the process of selecting a subset of original features that are most relevant to the prediction task, without transforming them.It keeps the original meaning of features intact.\n",
        "\n",
        "**Types of Feature Selection:**\n",
        "\n",
        "- Filter methods (correlation, chi-square)\n",
        "\n",
        "- Wrapper methods (forward selection, backward elimination)\n",
        "\n",
        "- Embedded methods (LASSO, tree-based feature importance)\n",
        "\n",
        "**Difference Between PCA and Feature Selection :**\n",
        "\n",
        "| Aspect | PCA | Feature Selection |\n",
        "|------|-----|------------------|\n",
        "| Approach | Feature extraction | Feature selection |\n",
        "| Type | Unsupervised | Usually supervised |\n",
        "| Output Features | New transformed features | Original features |\n",
        "| Interpretability | Low | High |\n",
        "| Uses Target Variable | No | Yes (often) |\n",
        "| Multicollinearity | Removes it | May retain it |\n",
        "| Example | PC1, PC2 | Age, Income |\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion** : PCA reduces dimensionality by creating new uncorrelated features that capture maximum variance, whereas feature selection reduces dimensionality by choosing the most relevant original features. PCA focuses on data representation, while feature selection focuses on feature relevance to the target variable.\n",
        "\n",
        "----\n"
      ],
      "metadata": {
        "id": "HMqQ48ROvG5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "--> Eigenvectors are **direction vectors** that define the new axes (principal\n",
        "components) in PCA. They indicate the **directions of maximum variance** in the\n",
        "data after transformation.In PCA, eigenvectors are obtained from the **covariance matrix** of the\n",
        "dataset.\n",
        "Eigenvalues are **scalar values** that correspond to each eigenvector. They\n",
        "represent the **amount of variance** captured along the direction of their\n",
        "associated eigenvector.\n",
        "\n",
        "- Large eigenvalue → More variance explained\n",
        "- Small eigenvalue → Less variance explained\n",
        "\n",
        "\n",
        "**Role of Eigenvalues and Eigenvectors in PCA:**\n",
        "\n",
        "1. **Direction of Data Spread**\n",
        "   - Eigenvectors define the directions along which data varies the most.\n",
        "\n",
        "2. **Variance Measurement**\n",
        "   - Eigenvalues quantify how much information (variance) is present in each\n",
        "     direction.\n",
        "\n",
        "3. **Principal Component Selection**\n",
        "   - Eigenvectors with the **largest eigenvalues** are selected as principal\n",
        "     components.\n",
        "\n",
        "4. **Dimensionality Reduction**\n",
        "   - By keeping only top eigenvectors, PCA reduces dimensions while preserving\n",
        "     maximum information.\n",
        "\n",
        "**Why are They Important in PCA?:**\n",
        "\n",
        "- Help identify the most important patterns in data\n",
        "- Reduce noise and redundancy\n",
        "- Remove multicollinearity\n",
        "- Improve computational efficiency\n",
        "- Enable data visualization in lower dimensions\n",
        "\n",
        "**Example:**\n",
        "If a dataset has:\n",
        "- Eigenvalue₁ = 5.2\n",
        "- Eigenvalue₂ = 1.1\n",
        "- Eigenvalue₃ = 0.2\n",
        "\n",
        "Then the first principal component (PC1) captures the most variance and is the\n",
        "most important.\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "In PCA, **eigenvectors define the directions of new feature axes**, while\n",
        "**eigenvalues indicate the importance of those directions**. Together, they\n",
        "allow PCA to reduce dimensionality while retaining the most meaningful\n",
        "information from the dataset.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "u2bJebBiw2Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "--> K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) are often used\n",
        "together because PCA helps overcome the limitations of KNN in high-dimensional\n",
        "data. While KNN relies on distance calculations, PCA reduces dimensionality and\n",
        "removes redundancy, making distance measures more meaningful.\n",
        "\n",
        "\n",
        "\n",
        "**Role of PCA in the Pipeline:**\n",
        "- Reduces the number of features by projecting data onto principal components.\n",
        "- Removes correlated and irrelevant features.\n",
        "- Mitigates the **curse of dimensionality**.\n",
        "- Preserves maximum variance with fewer dimensions.\n",
        "\n",
        "\n",
        "\n",
        "**Role of KNN in the Pipeline:**\n",
        "- Performs classification or regression using distance-based similarity.\n",
        "- Benefits from cleaner, lower-dimensional feature space.\n",
        "- Produces more accurate and faster predictions after PCA.\n",
        "\n",
        "\n",
        "**How They Complement Each Other:**\n",
        "\n",
        "1. Improved Distance Measurement\n",
        "- PCA ensures distances used by KNN are more meaningful by removing noise and\n",
        "  redundancy.\n",
        "\n",
        "2. Reduced Computational Cost\n",
        "- Fewer dimensions mean faster distance calculations in KNN.\n",
        "\n",
        "3. Better Generalization\n",
        "- PCA removes noisy features, reducing overfitting in KNN.\n",
        "\n",
        "4. Improved Accuracy\n",
        "- KNN performs better when irrelevant features are eliminated.\n",
        "\n",
        "\n",
        "\n",
        "**Typical PCA + KNN Pipeline:**\n",
        "1. Standardize the dataset.\n",
        "2. Apply PCA to reduce dimensionality.\n",
        "3. Train KNN on transformed data.\n",
        "4. Evaluate performance.\n",
        "\n",
        "\n",
        "**Example Use Cases:**\n",
        "- Image recognition\n",
        "- Text classification\n",
        "- Bioinformatics datasets\n",
        "- High-dimensional sensor data\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "PCA and KNN complement each other by combining dimensionality reduction with\n",
        "distance-based learning. PCA makes KNN more efficient and accurate by addressing\n",
        "the curse of dimensionality, while KNN leverages the reduced feature space to\n",
        "make reliable predictions.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CTPy5J9xxr7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "# scaling. Compare model accuracy in both cases.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# KNN WITHOUT Feature Scaling\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(\"Accuracy WITHOUT feature scaling:\", accuracy_no_scaling)\n",
        "\n",
        "\n",
        "# KNN WITH Feature Scaling\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy WITH feature scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PliVmX4Gydf7",
        "outputId": "faa92b34-93b4-4025-9ab6-480dd0870046"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT feature scaling: 0.7222222222222222\n",
            "Accuracy WITH feature scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "# ratio of each principal component.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model (keeping all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio for each principal component\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"PC{i}: {var:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UlyEjFyy9HR",
        "outputId": "ea378e3d-d317-43ba-e8f7-26f41f766409"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "# components). Compare the accuracy with the original dataset.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# KNN on Original Dataset\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "\n",
        "# KNN on PCA-transformed Dataset (Top 2 components)\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy using Original Dataset:\", accuracy_original)\n",
        "print(\"Accuracy using PCA-transformed Dataset (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb7IUZHEzGj0",
        "outputId": "fb6ac2d3-8144-4d8b-8fe0-21c3505d0f12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Original Dataset: 0.9629629629629629\n",
            "Accuracy using PCA-transformed Dataset (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "# manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# KNN with Euclidean Distance\n",
        "knn_euclidean = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='euclidean'\n",
        ")\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "\n",
        "# KNN with Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='manhattan'\n",
        ")\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy with Euclidean Distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtQgKh0c0HYj",
        "outputId": "576bff2c-7cc1-4257-cb01-3aac12bd7433"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9629629629629629\n",
            "Accuracy with Manhattan Distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "--> Gene expression datasets typically have **thousands of features (genes)** but **very few samples**.  \n",
        "- This causes **overfitting** in traditional machine learning models.  \n",
        "- Many features are **correlated** or irrelevant for classification.\n",
        "\n",
        "To address this, we can use a **pipeline combining PCA for dimensionality reduction and KNN for classification**.\n",
        "\n",
        "\n",
        "**Step 1: Use PCA to Reduce Dimensionality:**\n",
        "- Apply **Principal Component Analysis (PCA)** to transform the original high-dimensional data into a smaller set of **uncorrelated principal components**.  \n",
        "- PCA captures the directions of **maximum variance**, reducing noise and redundancy.  \n",
        "- This lowers the risk of overfitting while preserving the most important information.\n",
        "\n",
        "\n",
        "\n",
        "**Step 2: Decide How Many Components to Keep:**\n",
        "- **Explained Variance Ratio:**  \n",
        "  Calculate the **cumulative explained variance** for each principal component.  \n",
        "- **Rule of Thumb:**  \n",
        "  Retain the top components that explain **95–99% of the total variance**.  \n",
        "- This ensures dimensionality is reduced without losing critical information.\n",
        "\n",
        "\n",
        "**Step 3: Use KNN for Classification Post-PCA:**\n",
        "- Apply **feature scaling** to the PCA-transformed data.  \n",
        "- Train a **K-Nearest Neighbors (KNN) classifier** using the reduced-dimensional data.  \n",
        "- KNN is chosen because it is **non-parametric**, simple, and effective in moderate dimensions.\n",
        "\n",
        "\n",
        "\n",
        "**Step 4: Evaluate the Model:**\n",
        "- Split the data into **training and test sets** or use **cross-validation**.  \n",
        "- Use metrics suitable for imbalanced or biomedical data:\n",
        "  - **Accuracy**: Overall correct predictions  \n",
        "  - **Precision, Recall, F1-score**: Important when misclassification is costly  \n",
        "  - **ROC-AUC**: Evaluates separability between classes  \n",
        "- Optionally, plot a **confusion matrix** to visualize misclassifications.\n",
        "\n",
        "\n",
        "\n",
        "**Step 5: Justify the Pipeline to Stakeholders:**\n",
        "- **Reduced Overfitting:** PCA lowers feature dimensionality, improving generalization.  \n",
        "- **Interpretability:** KNN predictions are easy to explain (based on similarity to known patients).  \n",
        "- **Computational Efficiency:** Reduced dimensions speed up distance calculations in KNN.  \n",
        "- **Data-Driven Decisions:** Preserves biologically relevant variation while filtering noise.  \n",
        "- **Robustness:** The pipeline works well even when the dataset has far more features than samples, common in biomedical datasets.\n",
        "\n",
        "\n",
        "\n",
        "**Summary :**\n",
        "\n",
        "1. **PCA** reduces thousands of genes to a manageable number of principal components.  \n",
        "2. **Number of components** is chosen based on **explained variance** (95–99%).  \n",
        "3. **KNN** classifies patients in the reduced space, minimizing overfitting.  \n",
        "4. **Evaluation metrics** ensure robust assessment.  \n",
        "5. The pipeline is **stakeholder-friendly**, balancing accuracy, interpretability, and computational efficiency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "plk9qdXo1H76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Optional Python Skeleton for Implementation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# X = gene expression data, y = cancer type labels\n",
        "\n",
        "# 1. Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Apply PCA\n",
        "pca = PCA(n_components=0.95)  # retain 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 3. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict and Evaluate\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO31DQ_c15X7",
        "outputId": "3a064b5a-d991-433a-8581-645e8a2eec73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.86      0.92        14\n",
            "           2       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.94      0.95      0.94        36\n",
            "weighted avg       0.95      0.94      0.94        36\n",
            "\n"
          ]
        }
      ]
    }
  ]
}